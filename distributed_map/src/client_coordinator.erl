-module(client_coordinator).
-behaviour(gen_fsm).
-include("clusterdefs.hrl").
-include_lib("stdlib/include/ms_transform.hrl").

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% This file described the client coordinator.  It interfaces with user functions and the server coordinators
% that actually perform the computations.  A user calls a function to submit a job to the client coordinator
% which splits the job into small chunks and distributed them to the server coordinators.  The dynamic adding
% and subtracting of server coordinators is handled, and the fair distribution of the chunks of multiple
% submitted jobs is also performed in a round-robin fashion.
% Functions exist to submit, delete, and check on jobs, and to retrieve finished output data.
% When a job is submitted a unique reference is returned, and in the future when the job is a finished the
% process that submitted the job will receive a message indicating its completion.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%% Messages used in this module %%%%%%%
% -> server-to-client existence notification
%     {server_up, ServerPid}
% -> request chunks be sent to server coordinator
%     {send_chunks, ServerCoordinatorPid, NumChunks}
% -> abnormal termination exit message from server coordinator
%     {'EXIT', ServerPid, Reason}
% -> job submission from public function
%     {submit_job, Functions, InputData}
% -> get the status of a job
%     {job_status, Ref}
% -> retrieve a job
%     {get_job, Ref}
% -> set the chunk size
%     {set_chunk_size, Size}
% -> get the chunk size
%     {get_chunk_size}
% -> get the status of all jobs
%	  {all_job_status}
% -> get the list of connected servers
%     {get_connected_servers}
% -> calculation completion, sent to client coordinator by calculator module, but sometimes from
%    the server_coordinator in the case of complete calculation failure
%     {calc_done, ServerCoordinatorPid, OutChunk}
% <- client-to-server notification of having data ready to process
%     {client_data, Pid}
% <- outgoing chunks from client coordinator
%     {process_chunks, [Chunk1, Chunk2, ... ]}
% <- client coordinator to job submitting process completion notification
%     {job_complete, Ref}
%
% Here is the flow of messages between the client and server
% -> server_up			: notify all client coordinators of new server coordinator existence
% -------------
% <- client_data		: notify all server coordinators that client has data ready to be calculated
% -> send_chunks		: request client send server data
% <- process_chunks		: client sends server data with this message
% -> calc_done			: calculator sends client completed data with this message

%%%%%%% Data Structures used in this module %%%%%%%
% jobchunktables is a list with this structure:
% [Table1, Table2, Table3, ...]
% where Table1 is a reference to a sorted_set ETS table that has the following entries
% Chunk = {ClientCoordinatorPid, Ref, Seq, Functions, InputData}
% the key is the third field: Seq, and the value is the whole thing
% where InputData = [Data1, Data2, Data3, ...]
% where ClientCoordinatorPid is the PID of the originating client coordinator,
%   Ref is the job reference generated by that coordinator,
%   Seq is an integer sequence number,
%   Functions = {PreCalcFunctionMFA, CalcFunction, PostCalcFunctionMFA} are the functions to run,
%   and InputData is a list of the data elements to process
%
% outchunktables is a dict with this structure:
% key: Ref, value: Table
% where Table is a reference to a sorted_set ETS table that has the following entries
% {Seq, OutputData}
% the key is the first field: Seq, and the value is the whole thing
% where OutputData = [OutData1, OutData2, OutData3, ...]
% Note that OutData1 = CalcFunction(Data1)
% 
% submitted_jobs is a dictionary of the following form
% key: Ref, value: {SubmitterPid, InputTable, OutputTable, TotalChunks, ChunksReceived, PostCalculationMFA}
% where SubmitterPid is the Pid of the process that submitted the job (usually the erlang shell)
% and InputTable and OutputTable are references to ETS tables
% and PostCalculationMFA is a function to be run by the server coordinators to cleanup after a job has completed
%
% chunksinprogress is a dict with this structure:
% key: {ServerCoordinatorPid, Ref, Seq}, value: Chunk
%
% server_coordinators is a dict with this structure:
% key: ServerCoordinatorPid, value: number of chunks currently processing

%%%%%%%%%%%%%%%%%%%%%%%% Defines %%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%% Records %%%%%%%%%%%%%%%%%%%%%%%%

% FSM State
-record(state,{
  jobchunktables=[],
  outchunktables=dict:new(),
  chunksinprogress=dict:new(),
  server_coordinators=dict:new(),
  submitted_jobs=dict:new(),
  chunksize=?DEFAULT_CHUNK_SIZE
}).

%%%%%%%%%%%%%%%%%%%%%%%% Exports %%%%%%%%%%%%%%%%%%%%%%%%
% GEN_FSM
-export([start/1, start_link/1, init/1, handle_event/3, handle_sync_event/4, handle_info/3, terminate/3, code_change/4]).
-export([waiting/2, feeding/2]).
% public functions
-export([submit_job/3, check_job/2, remove_job/2, get_job/2, check_all_jobs/1, get_connected_servers/1, start_watcher/2, stop_watcher/1, set_chunk_size/2, get_chunk_size/1]).
% test functions
-export([test/0, test_make_chunks/0, test_store_data/0, test_client_coordinator/0, test_client_and_server_coordinators/0, test_client_and_server_coordinators_fast/0]).

%%%%%%%%%%%%%%%%%%%%%%%% GEN_FSM functions%%%%%%%%%%%%%%%%%%%%%%%%

start_link(ClusterName) ->
	gen_fsm:start_link(?MODULE,ClusterName,[]).

start(ClusterName) ->
	gen_fsm:start(?MODULE,ClusterName,[]).

init(ClusterName) ->
	% register the server using a unique name
	utils:register_client(ClusterName),
	% figure out what server coordinators are in the cluster (returns a list of PIDs)
	ServerCoordinatorPids = utils:get_servers(ClusterName),
    % add them to a newly-created state record and enter the waiting state
	{ok, waiting, add_servers(#state{},ServerCoordinatorPids) }.

%%%--------- Functions that handle asynchronous events

% handle a computation server coming online
handle_event({server_up, ServerCoordinatorPid}, StateName, State) ->
	gen_fsm:send_event(self(), {new_server, ServerCoordinatorPid}),
	gen_event:notify(event_handler, {self(), {server_up, ServerCoordinatorPid}}),
	{next_state, StateName, State};

% handle a request from a computation server to send it data chunks to process
handle_event({send_chunks, ServerCoordinatorPid, NumChunks}, StateName, State) ->
	% pop some chunks off the jobchunkstable
	{NewJobChunksTables,Chunks} = pop_chunks(State#state.jobchunktables, NumChunks),
	State1 = State#state{jobchunktables = NewJobChunksTables},
	% send the popped chunks to the computation server
	State2 = send_chunks(State1, ServerCoordinatorPid, Chunks),
	NextState = case NewJobChunksTables of
		% if there are no more chunks left, go to the waiting state
		[] ->
			waiting;
		% if there are still chunks left, make sure the system was, and stays, in the feeding state
		_ ->
			StateName = feeding % this will throw an exception if StateName /= feeding
	end,
	{next_state, NextState, State2};

% handle a message that a computation server has finished some calculations
handle_event({calc_done, ServerCoordinatorPid, {Ref, Seq, OutData}}, StateName, State) ->
	OutChunk = {Seq, OutData},
	% put the OutChunk in the outchunktable
	OutChunkTable = dict:fetch(Ref, State#state.outchunktables),
	% fetch the old job information
	{SubmitterPid, InputTable, OutputTable, TotalChunks, ChunksReceived, PostCalculationMFA} = dict:fetch(Ref, State#state.submitted_jobs),
	% insert the OutChunk into the OutChunkTable
	case ets:insert_new(OutChunkTable, OutChunk) of
		% if the OutChunk has not already been inserted to the OutChunkTable
		true ->
			% update submitted_jobs
			NewChunksReceived = ChunksReceived+1,
			% see if 10% more of the job has been completed, if so, then send out a notification
			case ( ((10*NewChunksReceived) div TotalChunks) - ((10*ChunksReceived) div TotalChunks) ) of
				1 -> gen_event:notify(event_handler, {self(), {progress_update, Ref, ChunksReceived, TotalChunks}});
				_ -> ok
			end,
			NewJob = {SubmitterPid, InputTable, OutputTable, TotalChunks, NewChunksReceived, PostCalculationMFA},
			NewSubmittedJobs = dict:store(Ref, NewJob, State#state.submitted_jobs);
		% otherwise, don't update chunks received information
		false ->
			NewChunksReceived = ChunksReceived,
			NewSubmittedJobs = State#state.submitted_jobs
	end,
	% remove the entry from the chunksinprogress dict
	NewChunksInProgress = dict:erase({ServerCoordinatorPid, Ref, Seq}, State#state.chunksinprogress),
	% update server_coordinators
	CurrentChunksNum = dict:fetch(ServerCoordinatorPid, State#state.server_coordinators),
	NewServerCoordinators = dict:store(ServerCoordinatorPid, CurrentChunksNum-1, State#state.server_coordinators),
	% check to see if ChunksReceived == TotalChunks
	case NewChunksReceived of
		% if so, then:
		TotalChunks ->
			% get a list of all server coordinators connected to this client coordinator.
			% NOTE: we can't use the get_connected_servers function because it requires an event to be processed, but we're currently processing an event
			% and things will deadlock if we try to call that function.  So we have to repeat the code, with a few changes, here.
			% an anonymous function to compile a list of server coordinators
			FoldHelper = fun
				(TempServerCoordinatorPid, _Chunks, Acc) ->
					 Acc ++ [TempServerCoordinatorPid]
			end,
			% fold over the list of all server coordinators and extract information about them
			ServerCoordinatorPids = dict:fold(FoldHelper, [], State#state.server_coordinators),
			% tell all of the server coordinators connected to this client coordinator to run this job's postcalculation function
			[ gen_fsm:send_all_state_event(Pid, {job_cleanup, {self(), Ref, PostCalculationMFA}}) || Pid <- ServerCoordinatorPids],
			% fire off an event
			gen_event:notify(event_handler, {self(), {job_complete, Ref}}),
			% notify the job submitter
			SubmitterPid ! {job_complete, Ref};
		% otherwise, don't do anything
		_ -> ok
	end,
	% set the new state
	NewState = State#state{
		chunksinprogress = NewChunksInProgress,
		server_coordinators = NewServerCoordinators,
		submitted_jobs = NewSubmittedJobs
	},
	{next_state, StateName, NewState};

% handle all other types of events by doing nothing
handle_event(_Event, StateName, State) ->
	{next_state, StateName, State}.

%%%--------------- Functions that handle synchronous events

% submit a job for the client coordinator
handle_sync_event({submit_job, Functions, InputData}, {SubmitterPid, _Tag}, StateName, State) ->
	% create a unique reference for the job
	Ref = make_ref(),
	% store data in an ETS table
	State1 = store_data(State, self(), SubmitterPid, Ref, Functions, InputData),
	% send a message to self that a new job has come in
	gen_fsm:send_event(self(),new_job),
	% fire off an event that a job has been submitted
	gen_event:notify(event_handler, {self(), {submit_job, SubmitterPid, Ref}}),
	% return the job reference
	{reply, Ref, StateName, State1};

% handle a request for that status of a job
handle_sync_event({job_status, Ref}, _FromTag, StateName, State) ->
	% set the return message
	Message = case dict:find(Ref, State#state.submitted_jobs) of
		% if the job can be found
		{ok, {_, _InputTable, _OutputTable, NumTotal, NumProcessed, _PostCalculationMFA}} ->
			"Processed " ++ integer_to_list(NumProcessed) ++ " out of " ++ integer_to_list(NumTotal) ++ " chunks";
		% otherwise, if it can't be found
     	error ->
			"Error: bad job reference"
	end,
	% return the message
	{reply,Message, StateName, State};

% handle a request to delete a job
handle_sync_event({delete_job, Ref}, _FromTag, StateName, State) ->
	case delete_job(Ref,State) of
		% if the job is found and deleted
		{ok, NewState} ->
			% fire off an event that the job has been deleted
			gen_event:notify(event_handler, {self(), {delete_job, Ref}}),
			{reply,"Deleted job", StateName, NewState};
		% if the job can't be found
		{bad_ref, NewState} ->
			{reply,"Error: bad job reference", StateName, NewState}
	end;

% handle a request to get data from a finished job
handle_sync_event({get_job, Ref}, _FromTag, StateName, State) ->
	case dict:find(Ref, State#state.submitted_jobs) of
		% if the job has finished
		{ok, {_, _InputTable, OutputTable, N, N, _}} -> % this pattern matches when NumTotal =:= NumProcessed
			% convert the outchunks to output data
			OutData = outdata_from_outchunks(OutputTable),
			% delete the job
			{ok, NewState} = delete_job(Ref, State),
			% fire off an event that the job data has been retrieved
			gen_event:notify(event_handler, {self(), {get_job, Ref}}),
			% send back the output data
			{reply,{ok, OutData},StateName,NewState};
		% otherwise if the job hasn't finished
		{ok, {_, _InputTable, _OutputTable, NumTotal, NumProcessed, _}} ->
			{reply,{not_finished,NumProcessed,NumTotal},StateName,State};
		% if Ref wasn't found in the table the job doesn't exist
		error ->
			{reply,bad_ref,StateName,State}
	end;

% get the status of all jobs
handle_sync_event(all_job_status, _FromTag, StateName, State) ->
	% an anonymous function to compile a list of job statuses
	FoldHelper = fun
		(Ref,{SubmitterPid, _InputTable, _OutputTable, TotalChunks, ChunksReceived, PostCalcFunctionMFA}, Acc) ->
			Acc ++ [{Ref, SubmitterPid, TotalChunks, ChunksReceived, PostCalcFunctionMFA}]
	end,
	% fold over the list of all jobs and extract certain status information about them
	{reply, dict:fold(FoldHelper, [], State#state.submitted_jobs), StateName, State};

% get a list of the compute servers that are connected to the client coordinator
handle_sync_event(get_connected_servers, _FromTag, StateName, State) ->
	% an anonymous function to compile a list of server coordinators and the chunks they're processing
	FoldHelper = fun
		(ServerCoordinatorPid, Chunks, Acc) ->
			 Acc ++ [{ServerCoordinatorPid, Chunks}]
	end,
	% fold over the list of all server coordinators and extract information about them
	{reply, dict:fold(FoldHelper, [], State#state.server_coordinators), StateName, State};

% set the chunk size used by the client coordinator
handle_sync_event({set_chunk_size, Size}, _FromTag, StateName, State) ->
	{reply, {ok, Size}, StateName, State#state{chunksize = Size}};

% get the chunk size being used by the client coordinator
handle_sync_event({get_chunk_size}, _FromTag, StateName, State) ->
	{reply, State#state.chunksize, StateName, State};

% handle all other events by doing nothing
handle_sync_event(_Event, _FromTag, StateName, State) ->
	{next_state, StateName, State}.

%%%--------------- Functions to handle non-gen_fsm messages

% if a server coordinator terminates for any reason
handle_info({'DOWN', _Ref, process, ServerCoordinatorPid, _Reason}, StateName, State) ->
	% fire off an event of the server's failure
	gen_event:notify(event_handler, {self(), {server_down, ServerCoordinatorPid}}),
	% check to see if there are any chunks being processed by that coordinator
	NewState = case dict:fetch(ServerCoordinatorPid, State#state.server_coordinators) of
		% if not, then return the original state
		0 -> State;
		% if so then:
		_ ->
			% retrieve all of the chunks it was working on from the chunksinprogress dict
			ChunksInProgress = get_chunks_in_progress(State#state.chunksinprogress, ServerCoordinatorPid),
			% send a new_job event to self
			gen_fsm:send_event(self(),new_job),
			% put those chunks back into the jobchunkstable in their appropriate spots, and return the modified state
			lists:foldl(fun put_chunk_back_in/2,State,ChunksInProgress)
	end,
	% remove the server coordinator from the server_coordinators list and continue
	{next_state, StateName, NewState#state{server_coordinators = dict:erase(ServerCoordinatorPid, NewState#state.server_coordinators)}};

% handle all other messages by doing nothing
handle_info(_Info, StateName, State) ->
	{next_state, StateName, State}.

%%%--------------- Functions that handle special gen_fsm operations

% terminate the gen_fsm
terminate(_Reason, _StateName, _State) ->
	ok.

% hot code swapping functionality doesn't do anything
code_change(_OldVersion, StateName, State, _Extra) ->
	{ok, StateName, State}.

%%%%%%%%%%%%%%%%%%%%%%%% GEN_FSM states and messages %%%%%%%%%%%%%%%%%%%%%%%%

% if a new job has come in while waiting
waiting(new_job, State) ->
	% notify all server coordinators that aren't currently processing chunks for the client that data is available
	UnoccupiedServers = unoccupied_servers(State#state.server_coordinators),
	notify_servers_of_data(UnoccupiedServers),
	% transition to the feeding state
	{next_state, feeding, State};
	
% if a new server comes online while waiting
waiting({new_server, ServerCoordinatorPid}, State) ->
	% add the new server to the dictionary, but do nothing more
	{next_state, waiting, add_servers(State, [ServerCoordinatorPid])}.

% if a new job has come in while feeding, don't do anything, as its chunks will automatically be processed
feeding(new_job, State) ->
	{next_state, feeding, State};

% if a new server comes online while feeding
feeding({new_server, ServerCoordinatorPid}, State) ->
	% notify the new server that we have data it can process
	notify_servers_of_data([ServerCoordinatorPid]),
	% add the new server to the dictionary and return the altered state
	{next_state, feeding, add_servers(State, [ServerCoordinatorPid])}.
	
%%%%%%%%%%%%%%%%%%%%%%%% Public functions %%%%%%%%%%%%%%%%%%%%%%%%

% submit a job to the client coordinator
submit_job(ClientCoordinatorPid, Functions, InputData) ->
	gen_fsm:sync_send_all_state_event(ClientCoordinatorPid, {submit_job, Functions, InputData}).

% check the status of a job
check_job(ClientCoordinatorPid, Ref) ->
	gen_fsm:sync_send_all_state_event(ClientCoordinatorPid, {job_status, Ref}).

% delete a job from the client coordinator
remove_job(ClientCoordinatorPid, Ref) ->
	gen_fsm:sync_send_all_state_event(ClientCoordinatorPid, {delete_job, Ref}).

% get the completed contents of a job
get_job(ClientCoordinatorPid, Ref) ->
	gen_fsm:sync_send_all_state_event(ClientCoordinatorPid, {get_job, Ref}, infinity).

% check the state of all jobs being handled by the client coordinator
% returns list of {Ref, SubmittedPid, TotalChunks, ChunksReceived} tuples
check_all_jobs(ClientCoordinatorPid) ->
	gen_fsm:sync_send_all_state_event(ClientCoordinatorPid, all_job_status).

% get a list of all computation servers connected to the client coordinator
% returns list of {ServerCoordinatorPid, ChunksSubmitted} tuples
get_connected_servers(ClientCoordinatorPid) ->
	gen_fsm:sync_send_all_state_event(ClientCoordinatorPid, get_connected_servers).

% start a gen_server process that attaches to the gen_event process, and start sending status
% updates to the starter's console
start_watcher(ClientCoordinatorPid, JobRef) ->
	{ok, WatcherPid} = gen_server:start_link(client_watcher, [], []),
	HandlerId = {client_events, make_ref()},
	gen_event:add_handler(event_handler, HandlerId, [WatcherPid, ClientCoordinatorPid, JobRef]),
	HandlerId.

% stop watching the gen_server process, and remove the event handler from the event notifier process
stop_watcher(HandlerId) ->
	gen_event:delete_handler(event_handler, HandlerId, done).

% set the chunk size that will be used by the client coordinator
set_chunk_size(ClientCoordinatorPid, Size) ->
	if
		% if the given size is valid, send a message to the FSM to adjust it
		Size > 0 -> gen_fsm:sync_send_all_state_event(ClientCoordinatorPid, {set_chunk_size, Size});
		% otherwise, return bad_size		
	    true -> bad_size
	end.

% get the chunk size that is being used by the client coordinator
get_chunk_size(ClientCoordinatorPid) ->
	gen_fsm:sync_send_all_state_event(ClientCoordinatorPid, {get_chunk_size}).

%%%%%%%%%%%%%%%%%%%%%%%% Private functions %%%%%%%%%%%%%%%%%%%%%%%%

% Split data into chunks and enumerate those chunks
% returns Chunks
make_chunks(ChunkSize, ClientCoordinatorPid, Ref, Functions, InputData) ->
	make_chunks_helper(ChunkSize, ClientCoordinatorPid, Ref, 0, Functions, InputData, length(InputData), []).

% recursive helper function for make_chunks
% this function matches when the InputData list has been exhausted and returns the chunks accumulator
make_chunks_helper(_ChunkSize, _ClientCoordinatorPid, _Ref, _Counter, _Functions, [], 0, Acc) ->
	Acc;
% this function matches when the amount of data left is smaller than the chunk size
make_chunks_helper(ChunkSize, ClientCoordinatorPid, Ref, Counter, Functions, Data, DataLength, Acc) when DataLength < ChunkSize ->
	make_chunks_helper(ChunkSize, ClientCoordinatorPid, Ref, Counter+1, Functions, [], 0, [{ClientCoordinatorPid, Ref, Counter, Functions, Data}] ++ Acc);
% this function matches in all other cases and recursively calls itself
make_chunks_helper(ChunkSize, ClientCoordinatorPid, Ref, Counter, Functions, Data, DataLength, Acc) ->
	% split off a ChunkSize piece of data from the list
	{DataChunk, DataRemaining} = lists:split(ChunkSize, Data),
	% recursively call self, adding the DataChunk onto the accumulator and processing the DataRemaining
	make_chunks_helper(ChunkSize, ClientCoordinatorPid, Ref, Counter+1, Functions, DataRemaining, DataLength-ChunkSize, [{ClientCoordinatorPid, Ref, Counter, Functions, DataChunk}] ++ Acc).

% store data in an ETS table.  For really large data sets, storing them in a dictionary becomes very inefficient and Erlang becomes very slow
store_data(State, ClientCoordinatorPid, SubmitterPid, Ref, Functions, InputData) ->
	% split the data into chunks
	Chunks = make_chunks(State#state.chunksize, ClientCoordinatorPid, Ref, Functions, InputData),
	% create a new ETS table for the data.  Although the name "inputchunks" appears, this is not a named table, so the important
	% identifier is the value returned and stored in InputTable.  Thus an ETS table is created for each job submitted
	% note that this is an ordered_set, so the OutputChunks received from the computation nodes will be stored in-order and can easily
	% be converted into OutputData that has the same order as the InputData
	InputTable = ets:new(inputchunks, [ordered_set,{keypos, 3}]),
	% store the chunks in an ETS table, using the counter as the key
	lists:map(fun(Chunk) -> ets:insert(InputTable, Chunk) end, Chunks),
	% also create a new ETS table for the output chunks
	OutputTable = ets:new(outputchunks, [ordered_set]),
	% extract the PostCalcFunctionMFA from Functions
	{_, _, PostCalcFunctionMFA} = Functions,
	% store all of this new information in the state
	State#state{
		jobchunktables = State#state.jobchunktables ++ [InputTable],
		outchunktables = dict:store(Ref,OutputTable,State#state.outchunktables),
		submitted_jobs = dict:store(Ref, {SubmitterPid, InputTable, OutputTable, length(Chunks), 0, PostCalcFunctionMFA}, State#state.submitted_jobs)
	}.

% returns OutputData from the chunks stored in the OutputTable
outdata_from_outchunks(OutputTable) ->
	% fold over the chunks in the OutputTable.  Since they are stored in order by the ETS table, no sorting is necessary
	% Note, we do a right fold because Outputdata ++ Acc is much faster for large lists than a left fold with Acc ++ Outputdata because Erlang must store lists as linked-lists starting from the left, so to concatenate Acc and OutputData requires traversing Acc for every concatenation.  Kind of dumb.
	ets:foldr(fun({_Seq,OutputData},Acc) -> OutputData ++ Acc end, [], OutputTable).

% delete a job
% returns {ok, State} or {bad_ref, State}
delete_job(Ref, State) ->
	% retrieve the job information
	TempJob = dict:find(Ref, State#state.submitted_jobs),
	case TempJob of
		% if the job exists, then delete its ETS tables
		{ok, {_SubmitterPid, InputTable, OutputTable, _NumTotal, _NumProcessed, _PostCalcFunctionMFA}} ->
			% delete the input and output tables
			ets:delete(InputTable),
			ets:delete(OutputTable),
			% store the new state by erasing the job from the various dictionaries
			NewState = State#state{
		   		jobchunktables = lists:delete(InputTable, State#state.jobchunktables),
				outchunktables = dict:erase(Ref, State#state.outchunktables),
				submitted_jobs = dict:erase(Ref, State#state.submitted_jobs)
			},
			{ok, NewState};
		% if the job doesn't exist, then return an error
		error ->
			{bad_ref, State}
	end.

% add one or more server coordinators to the list of server coordinators handling jobs from this client coordinator
% returns State
add_servers(State, ServerCoordinatorPids) ->
	% monitor the server coordinators
	lists:map(fun(Pid) -> erlang:monitor(process, Pid) end, ServerCoordinatorPids),
	% put these into the server_coordinators dictionary along with 0 chunks being worked on
	ServerCoordinators = lists:foldl(fun(Pid,Dict) -> dict:store(Pid, 0, Dict) end, State#state.server_coordinators, ServerCoordinatorPids),
	% return the modified state
	State#state{server_coordinators = ServerCoordinators}.

% notify the connected computation servers of data that is ready to be processed
% return value is not important
notify_servers_of_data(ServerCoordinatorPids) ->
	lists:map(fun(Pid) -> gen_fsm:send_all_state_event(Pid, {client_data, self()}) end, ServerCoordinatorPids).

% find the server coordinators that aren't working on any chunks for the client coordinators
% returns list of Pids
unoccupied_servers(ServerCoordinators) ->
	% an anonymous function that returns a list of server coordinators that aren't working on any jobs
	FoldHelper = fun
		(_ServerCoordinatorPid, NumProcessing, Acc) when NumProcessing > 0->
			Acc;
		(ServerCoordinatorPid, 0, Acc) ->
			Acc ++ [ServerCoordinatorPid]
	end,
	dict:fold(FoldHelper, [], ServerCoordinators).

% pop a number of chunks off the ETS tables in a round-robin fashion
% returns {JobChunksTables, Chunks}
pop_chunks(JobChunksTables, Number) ->
	pop_chunks_helper(JobChunksTables, Number, Number, []).

% recursive helper function for the pop_chunks function
% if all of the chunks have been popped, then return what we have so far
pop_chunks_helper([], _Number, _Counter, Acc) ->
	{[], Acc};
% if we have popped the necessary number of chunks, then return what we have
pop_chunks_helper(JobChunksTables, _Number, 0, Acc) ->
	{JobChunksTables, Acc};
% otherwise, do the following
pop_chunks_helper([FirstTable|OtherTables], Number, Counter, Acc) ->
	% try to get the first item in the ETS table
	case ets:first(FirstTable) of
		% if the first table is empty, then remove it from the round-robin list of JobChunksTables and keep the recursive call going
		'$end_of_table' ->
			pop_chunks_helper(OtherTables, Number, Counter, Acc);
		% if there's a chunk on the first table in the list...
		Key ->
			% get the chunk, delete it from the database, then keep the recursive call going
			[Chunk] = ets:lookup(FirstTable, Key),
			ets:delete(FirstTable, Key),
			% put the first table at the end of the OtherTables, thus doing round-robin processing of the data in the JobChunksTables
			pop_chunks_helper(OtherTables ++ [FirstTable], Number, Counter-1, [Chunk] ++ Acc)
	end.

% send chunks to the given server coordinator
% returns State
send_chunks(State, ServerCoordinatorPid, Chunks) ->
	% store the chunks in the chunksinprogress dict
	NewChunksInProgress = lists:foldl(fun({_ClientCoordinatorPid, Ref, Seq, _Functions, _Data} = Chunk, Acc) -> dict:store({ServerCoordinatorPid, Ref, Seq}, Chunk, Acc) end, State#state.chunksinprogress, Chunks),
	% increase the number of chunks that are currently being processed by the server coordinator
	OldNum = dict:fetch(ServerCoordinatorPid, State#state.server_coordinators),
	NewServerCoordinators = dict:store(ServerCoordinatorPid, OldNum + length(Chunks), State#state.server_coordinators),
	% send the chunks to the server coordinator
	gen_fsm:send_all_state_event(ServerCoordinatorPid, {process_chunks,Chunks}),
	% set the new state
	State#state{
		chunksinprogress = NewChunksInProgress,
		server_coordinators = NewServerCoordinators
	}.

% find all the chunks that were being processed by ServerCoordinatorPid
% returns Chunks
get_chunks_in_progress(ChunksInProgressDict, ServerCoordinatorPid) ->
	% anonymous helper function adds a Chunk onto an accumulator when the chunk's Pid matches the ServerCoordinatorPid
	FoldHelper = fun
		({Pid, _, _}, Chunk, Acc) when Pid =:= ServerCoordinatorPid->
			[Chunk] ++ Acc;
		(_,_,Acc) -> Acc
	end,
	% fold over all chunks in progress and extract the ones matching the ServerCoordinatorPid
	dict:fold(FoldHelper, [], ChunksInProgressDict). 	

% in the event of a server coordinator failure, the chunks being processed by that coordinator should be put back to be processed
% by another server coordinator in the future
% returns State
put_chunk_back_in({_ClientCoordinatorPid, Ref, _Seq, _Functions, _InputData} = Chunk, State) ->
	% get the InputTable for the chunk
	{_,InputTable,_,_,_,_} = dict:fetch(Ref, State#state.submitted_jobs),
	% put the chunk back in its InputTable
	ets:insert(InputTable, Chunk),
	% check to see if the InputTable is already in the jobchunktables list
	case lists:member(InputTable,State#state.jobchunktables) of
		% if so, then return the original state as the returned chunk will eventually be processed
		true -> State;
		% if not, then put it in the jobchunktables list to restart its computation and return the modified state
		false -> State#state{jobchunktables = State#state.jobchunktables ++ [InputTable]}
	end.

%%%%%%%%%%%%%%%%%%%%%%%% Test functions %%%%%%%%%%%%%%%%%%%%%%%%

test() ->
	io:format("--== test_make_chunks ==--~n"),
	erlang:display(test_make_chunks()),
	io:format("--== test_store_data ==--~n"),
	erlang:display(test_store_data()),
	io:format("--== test_client_coordinator ==--~n"),
	test_client_coordinator(),
%% 	io:format("--== test_client_and_server_coordinators ==--~n"),
%% 	test_client_and_server_coordinators(),
	io:format("--== test_client_and_server_coordinators_fast ==--~n"),
	test_client_and_server_coordinators_fast().

test_make_chunks() ->
	make_chunks(10,self(),make_ref(),fun(X) -> X+1 end, lists:seq(1,25)).

test_store_data() ->
	NewState = store_data(#state{}, self(), self(), make_ref(), fun(X) -> X+1 end, lists:seq(1,2*?DEFAULT_CHUNK_SIZE)),
	Table = hd(NewState#state.jobchunktables),
	ets:match(Table,'$1').

test_client_coordinator() ->
	io:format("For running this test, set NUMCHUNKS to 2~n"),
	ClusterName = testcluster,
	process_flag(trap_exit, true),
	{ok, Name} = utils:register_server(ClusterName),
	{ok, ClientCoordinatorPid} = client_coordinator:start_link(ClusterName),
	Ref = submit_job(ClientCoordinatorPid, fun(X) -> X+1 end, [1,2,3,4]),
	erlang:display(check_job(ClientCoordinatorPid, Ref)),
	erlang:display(get_job(ClientCoordinatorPid, make_ref())),
	erlang:display(get_job(ClientCoordinatorPid, Ref)),
	erlang:display(remove_job(ClientCoordinatorPid, make_ref())),
	erlang:display(remove_job(ClientCoordinatorPid, Ref)),
	Ref2 = submit_job(ClientCoordinatorPid, fun(X) -> X+1 end, [1,2,3,4]),
	receive
		{'$gen_all_state_event',{client_data,Pid}} -> Pid = ClientCoordinatorPid
	end,
	gen_fsm:send_all_state_event(ClientCoordinatorPid, {send_chunks, self(), 10}),
	receive
		{'$gen_all_state_event',{process_chunks, Chunks}} -> ok
	end,
	[calculator:calculate(self(), Chunk) || Chunk <- Chunks],
	receive
		{job_complete, JobRef} -> JobRef = Ref2
	end,
 	erlang:display(get_job(ClientCoordinatorPid, Ref2)),
 	erlang:display(remove_job(ClientCoordinatorPid, Ref2)),


	Ref3 = submit_job(ClientCoordinatorPid, fun(X) -> X+1 end, [1,2,3,4]),
	receive
		{'$gen_all_state_event',{client_data,Pid2}} -> Pid2 = ClientCoordinatorPid
	end,
	gen_fsm:send_all_state_event(ClientCoordinatorPid, {send_chunks, self(), 10}),
	receive
		{'$gen_all_state_event',{process_chunks, Chunks2}} -> ok
	end,
	[calculator:calculate(self(), Chunk) || Chunk <- Chunks2],
	receive
		{job_complete, JobRef2} -> JobRef2 = Ref3
	end,
 	erlang:display(get_job(ClientCoordinatorPid, Ref3)),
 	erlang:display(remove_job(ClientCoordinatorPid, Ref3)),


	global:unregister_name(Name).

test_client_and_server_coordinators() ->
	ClusterName = testcluster,
	process_flag(trap_exit, true),
	{ok, ServerCoordinatorPid} = server_coordinator:start_link(ClusterName),
	{ok, ClientCoordinatorPid} = client_coordinator:start_link(ClusterName),
	server_coordinator:change_worker_number(10, ServerCoordinatorPid),
	io:format("--== calculation1_with_exit ==--~n"),
	Ref = submit_job(ClientCoordinatorPid, fun calculator:calculation1_with_exit/1, lists:seq(1,100)),
	timer:sleep(2000),
	erlang:display(check_job(ClientCoordinatorPid, Ref)),
 	erlang:display(get_job(ClientCoordinatorPid, Ref)),
	%%%%%%%%%%%%%%%%%%%%%
	io:format("--== calculation1_with_exit repeat ==--~n"),
	Ref2 = submit_job(ClientCoordinatorPid, fun calculator:calculation1_with_exit/1, lists:seq(1,100)),
	timer:sleep(2000),
	erlang:display(check_job(ClientCoordinatorPid, Ref2)),
 	erlang:display(get_job(ClientCoordinatorPid, Ref2)),
	%%%%%%%%%%%%%%%%%%%%%
	io:format("--== calculation1_with_sleep ==--~n"),
	Ref3 = submit_job(ClientCoordinatorPid, fun calculator:calculation1_with_sleep/1, lists:seq(1,100)),
	timer:sleep(5000),
	erlang:display(check_job(ClientCoordinatorPid, Ref3)),
	timer:sleep(5000),
	erlang:display(check_job(ClientCoordinatorPid, Ref3)),
	timer:sleep(5000),
	erlang:display(check_job(ClientCoordinatorPid, Ref3)),
 	erlang:display(get_job(ClientCoordinatorPid, Ref3)).

test_client_and_server_coordinators_fast() ->
	ClusterName = testcluster,
	process_flag(trap_exit, true),
	{ok, ServerCoordinatorPid} = server_coordinator:start_link(ClusterName),
	{ok, ClientCoordinatorPid} = client_coordinator:start_link(ClusterName),
	server_coordinator:change_worker_number(2, ServerCoordinatorPid),
	Ref = submit_job(ClientCoordinatorPid, fun calculator:calculation1/1, lists:seq(1,100)),
	receive
		{job_complete, JobRef} -> JobRef = Ref
	end,
 	erlang:display(get_job(ClientCoordinatorPid, Ref)).
